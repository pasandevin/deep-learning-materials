{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "pip install -U keras-tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional (Only if errors appear)\n",
    "\n",
    "1. pip install tensorflow --upgrade --user\n",
    "2. pip install keras --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SEX  AGEIR   TC  HDL  SMOKE_  BPMED  DIAB_01  RISK\n",
      "0    1     48  236   66       0      1        0   1.1\n",
      "1    0     48  260   51       0      1        1   7.0\n",
      "2    0     44  187   49       1      1        0   7.0\n",
      "3    1     42  216   57       1      1        0   0.4\n",
      "4    1     56  156   42       0      1        0   2.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset=pd.read_csv('cardio_dataset.csv')\n",
    "print(dataset.head())\n",
    "dataset=dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=dataset[:,0:7]\n",
    "target=dataset[:,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "target=np.reshape(target, (-1,1))\n",
    "\n",
    "scaler_data = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "data_scaled=scaler_data.fit_transform(data)\n",
    "target_scaled=scaler_target.fit_transform(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data, train_target, test_target = train_test_split(data_scaled, target_scaled,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "import numpy as np\n",
    "\n",
    "from keras.optimizers import Adam,Adagrad,Adadelta\n",
    "\n",
    "def build_model(parameters):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    for i in range(parameters.Int('num_layers', 2, 20)):\n",
    "\n",
    "        if(i==0):\n",
    "            model.add(Dense(units=parameters.Int('#neurons layer' + str(i),min_value=32,max_value=512,step=32),\n",
    "                            activation=parameters.Choice('activation_function '+str(i),['relu','sigmoid','tanh']),input_dim=7))\n",
    "            \n",
    "            model.add(Dropout(parameters.Choice('drop_prob '+str(i),[0.2,0.3,0.4,0.5])))\n",
    "        \n",
    "        else:\n",
    "            model.add(Dense(units=parameters.Int('#neurons layer' + str(i),min_value=32,max_value=512,step=32),\n",
    "                                   activation=parameters.Choice('activation_function '+str(i),['relu','sigmoid','tanh'])))\n",
    "            \n",
    "            model.add(Dropout(parameters.Choice('drop_prob '+str(i),[0.2,0.3,0.4,0.5])))\n",
    "            \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    model.compile(optimizer=parameters.Choice('optmz', ['adam', 'adaDelta', 'adaGrad']),loss=parameters.Choice('loss f', ['mse', 'mae']))\n",
    "    #model.compile(optimizer=Adam(parameters.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),loss='mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project project\\heart-risk\\oracle.json\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "tuner = RandomSearch(build_model,objective='val_loss',max_trials=5,executions_per_trial=3,directory='project',\n",
    "                     project_name='heart-risk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[more info](https://keras-team.github.io/keras-tuner/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 10\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 20, 'step': 1, 'sampling': None}\n",
      "#neurons layer0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': None}\n",
      "activation_function 0 (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'sigmoid', 'tanh'], 'ordered': False}\n",
      "drop_prob 1 (Choice)\n",
      "{'default': 0.2, 'conditions': [], 'values': [0.2, 0.3, 0.4, 0.5], 'ordered': True}\n",
      "#neurons layer1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': None}\n",
      "activation_function 1 (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'sigmoid', 'tanh'], 'ordered': False}\n",
      "drop_prob 2 (Choice)\n",
      "{'default': 0.2, 'conditions': [], 'values': [0.2, 0.3, 0.4, 0.5], 'ordered': True}\n",
      "optmz (Choice)\n",
      "{'default': 'adam', 'conditions': [], 'values': ['adam', 'adaDelta', 'adaGrad'], 'ordered': False}\n",
      "loss f (Choice)\n",
      "{'default': 'mse', 'conditions': [], 'values': ['mse', 'mae'], 'ordered': False}\n",
      "drop_prob 0 (Choice)\n",
      "{'default': 0.2, 'conditions': [], 'values': [0.2, 0.3, 0.4, 0.5], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 01m 09s]\n",
      "val_loss: 0.022785576681296032\n",
      "\n",
      "Best val_loss So Far: 0.0051856329664587975\n",
      "Total elapsed time: 00h 03m 02s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(train_data,train_target,epochs=10,validation_data=(test_data, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in project\\heart-risk\n",
      "Showing 10 best trials\n",
      "Objective(name='val_loss', direction='min')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "#neurons layer0: 352\n",
      "activation_function 0: tanh\n",
      "drop_prob 1: 0.2\n",
      "#neurons layer1: 480\n",
      "activation_function 1: sigmoid\n",
      "drop_prob 2: 0.3\n",
      "optmz: adam\n",
      "loss f: mse\n",
      "drop_prob 0: 0.2\n",
      "#neurons layer2: 288\n",
      "activation_function 2: relu\n",
      "#neurons layer3: 448\n",
      "activation_function 3: tanh\n",
      "drop_prob 3: 0.4\n",
      "#neurons layer4: 480\n",
      "activation_function 4: tanh\n",
      "drop_prob 4: 0.4\n",
      "#neurons layer5: 96\n",
      "activation_function 5: sigmoid\n",
      "drop_prob 5: 0.4\n",
      "#neurons layer6: 320\n",
      "activation_function 6: relu\n",
      "drop_prob 6: 0.4\n",
      "#neurons layer7: 128\n",
      "activation_function 7: relu\n",
      "drop_prob 7: 0.2\n",
      "#neurons layer8: 160\n",
      "activation_function 8: relu\n",
      "drop_prob 8: 0.2\n",
      "#neurons layer9: 416\n",
      "activation_function 9: tanh\n",
      "drop_prob 9: 0.2\n",
      "#neurons layer10: 64\n",
      "activation_function 10: tanh\n",
      "drop_prob 10: 0.4\n",
      "#neurons layer11: 480\n",
      "activation_function 11: relu\n",
      "drop_prob 11: 0.5\n",
      "#neurons layer12: 320\n",
      "activation_function 12: relu\n",
      "drop_prob 12: 0.4\n",
      "#neurons layer13: 192\n",
      "activation_function 13: sigmoid\n",
      "drop_prob 13: 0.3\n",
      "Score: 0.0051856329664587975\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 20\n",
      "#neurons layer0: 352\n",
      "activation_function 0: sigmoid\n",
      "drop_prob 1: 0.5\n",
      "#neurons layer1: 128\n",
      "activation_function 1: sigmoid\n",
      "drop_prob 2: 0.5\n",
      "optmz: adam\n",
      "loss f: mse\n",
      "drop_prob 0: 0.4\n",
      "#neurons layer2: 96\n",
      "activation_function 2: relu\n",
      "#neurons layer3: 416\n",
      "activation_function 3: tanh\n",
      "drop_prob 3: 0.3\n",
      "#neurons layer4: 320\n",
      "activation_function 4: tanh\n",
      "drop_prob 4: 0.5\n",
      "#neurons layer5: 192\n",
      "activation_function 5: sigmoid\n",
      "drop_prob 5: 0.3\n",
      "#neurons layer6: 480\n",
      "activation_function 6: relu\n",
      "drop_prob 6: 0.3\n",
      "#neurons layer7: 480\n",
      "activation_function 7: sigmoid\n",
      "drop_prob 7: 0.2\n",
      "#neurons layer8: 256\n",
      "activation_function 8: sigmoid\n",
      "drop_prob 8: 0.2\n",
      "#neurons layer9: 128\n",
      "activation_function 9: relu\n",
      "drop_prob 9: 0.3\n",
      "#neurons layer10: 480\n",
      "activation_function 10: relu\n",
      "drop_prob 10: 0.5\n",
      "#neurons layer11: 352\n",
      "activation_function 11: sigmoid\n",
      "drop_prob 11: 0.4\n",
      "#neurons layer12: 480\n",
      "activation_function 12: tanh\n",
      "drop_prob 12: 0.4\n",
      "#neurons layer13: 160\n",
      "activation_function 13: sigmoid\n",
      "drop_prob 13: 0.4\n",
      "#neurons layer14: 32\n",
      "activation_function 14: relu\n",
      "drop_prob 14: 0.2\n",
      "#neurons layer15: 32\n",
      "activation_function 15: relu\n",
      "drop_prob 15: 0.2\n",
      "#neurons layer16: 32\n",
      "activation_function 16: relu\n",
      "drop_prob 16: 0.2\n",
      "#neurons layer17: 32\n",
      "activation_function 17: relu\n",
      "drop_prob 17: 0.2\n",
      "#neurons layer18: 32\n",
      "activation_function 18: relu\n",
      "drop_prob 18: 0.2\n",
      "#neurons layer19: 32\n",
      "activation_function 19: relu\n",
      "drop_prob 19: 0.2\n",
      "Score: 0.022785576681296032\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 7\n",
      "#neurons layer0: 352\n",
      "activation_function 0: tanh\n",
      "drop_prob 1: 0.4\n",
      "#neurons layer1: 128\n",
      "activation_function 1: tanh\n",
      "drop_prob 2: 0.3\n",
      "optmz: adaDelta\n",
      "loss f: mse\n",
      "drop_prob 0: 0.5\n",
      "#neurons layer2: 320\n",
      "activation_function 2: relu\n",
      "#neurons layer3: 352\n",
      "activation_function 3: sigmoid\n",
      "drop_prob 3: 0.3\n",
      "#neurons layer4: 352\n",
      "activation_function 4: tanh\n",
      "drop_prob 4: 0.4\n",
      "#neurons layer5: 448\n",
      "activation_function 5: sigmoid\n",
      "drop_prob 5: 0.5\n",
      "#neurons layer6: 352\n",
      "activation_function 6: relu\n",
      "drop_prob 6: 0.3\n",
      "#neurons layer7: 128\n",
      "activation_function 7: tanh\n",
      "drop_prob 7: 0.3\n",
      "#neurons layer8: 288\n",
      "activation_function 8: sigmoid\n",
      "drop_prob 8: 0.2\n",
      "#neurons layer9: 352\n",
      "activation_function 9: sigmoid\n",
      "drop_prob 9: 0.3\n",
      "#neurons layer10: 160\n",
      "activation_function 10: relu\n",
      "drop_prob 10: 0.3\n",
      "#neurons layer11: 32\n",
      "activation_function 11: tanh\n",
      "drop_prob 11: 0.4\n",
      "#neurons layer12: 384\n",
      "activation_function 12: relu\n",
      "drop_prob 12: 0.4\n",
      "#neurons layer13: 64\n",
      "activation_function 13: sigmoid\n",
      "drop_prob 13: 0.5\n",
      "Score: 0.027186353380481403\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "#neurons layer0: 480\n",
      "activation_function 0: sigmoid\n",
      "drop_prob 1: 0.3\n",
      "#neurons layer1: 416\n",
      "activation_function 1: relu\n",
      "drop_prob 2: 0.5\n",
      "optmz: adaDelta\n",
      "loss f: mse\n",
      "drop_prob 0: 0.4\n",
      "#neurons layer2: 96\n",
      "activation_function 2: tanh\n",
      "#neurons layer3: 256\n",
      "activation_function 3: relu\n",
      "drop_prob 3: 0.5\n",
      "#neurons layer4: 512\n",
      "activation_function 4: tanh\n",
      "drop_prob 4: 0.5\n",
      "#neurons layer5: 160\n",
      "activation_function 5: relu\n",
      "drop_prob 5: 0.2\n",
      "#neurons layer6: 64\n",
      "activation_function 6: tanh\n",
      "drop_prob 6: 0.5\n",
      "#neurons layer7: 160\n",
      "activation_function 7: sigmoid\n",
      "drop_prob 7: 0.3\n",
      "#neurons layer8: 352\n",
      "activation_function 8: relu\n",
      "drop_prob 8: 0.4\n",
      "#neurons layer9: 416\n",
      "activation_function 9: tanh\n",
      "drop_prob 9: 0.5\n",
      "#neurons layer10: 160\n",
      "activation_function 10: tanh\n",
      "drop_prob 10: 0.4\n",
      "#neurons layer11: 512\n",
      "activation_function 11: relu\n",
      "drop_prob 11: 0.3\n",
      "#neurons layer12: 480\n",
      "activation_function 12: tanh\n",
      "drop_prob 12: 0.3\n",
      "#neurons layer13: 32\n",
      "activation_function 13: sigmoid\n",
      "drop_prob 13: 0.4\n",
      "Score: 0.032208324099580445\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 14\n",
      "#neurons layer0: 448\n",
      "activation_function 0: tanh\n",
      "drop_prob 1: 0.5\n",
      "#neurons layer1: 288\n",
      "activation_function 1: tanh\n",
      "drop_prob 2: 0.2\n",
      "optmz: adam\n",
      "loss f: mae\n",
      "drop_prob 0: 0.2\n",
      "#neurons layer2: 32\n",
      "activation_function 2: relu\n",
      "#neurons layer3: 32\n",
      "activation_function 3: relu\n",
      "drop_prob 3: 0.2\n",
      "#neurons layer4: 32\n",
      "activation_function 4: relu\n",
      "drop_prob 4: 0.2\n",
      "#neurons layer5: 32\n",
      "activation_function 5: relu\n",
      "drop_prob 5: 0.2\n",
      "#neurons layer6: 32\n",
      "activation_function 6: relu\n",
      "drop_prob 6: 0.2\n",
      "#neurons layer7: 32\n",
      "activation_function 7: relu\n",
      "drop_prob 7: 0.2\n",
      "#neurons layer8: 32\n",
      "activation_function 8: relu\n",
      "drop_prob 8: 0.2\n",
      "#neurons layer9: 32\n",
      "activation_function 9: relu\n",
      "drop_prob 9: 0.2\n",
      "#neurons layer10: 32\n",
      "activation_function 10: relu\n",
      "drop_prob 10: 0.2\n",
      "#neurons layer11: 32\n",
      "activation_function 11: relu\n",
      "drop_prob 11: 0.2\n",
      "#neurons layer12: 32\n",
      "activation_function 12: relu\n",
      "drop_prob 12: 0.2\n",
      "#neurons layer13: 32\n",
      "activation_function 13: relu\n",
      "drop_prob 13: 0.2\n",
      "Score: 0.07793246954679489\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 352)               2816      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 352)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 480)               169440    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 480)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 288)               138528    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 448)               129472    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 449       \n",
      "=================================================================\n",
      "Total params: 440,705\n",
      "Trainable params: 440,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "best_models = tuner.get_best_models()\n",
    "print(best_models[0].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
